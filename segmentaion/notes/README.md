# Multi-Scale Context Aggregation by Dilated Convolutions

## 论文信息

发表时间： 23 Nov 2015

论文地址：[Arxiv Link](https://arxiv.org/abs/1511.07122)

## 创新点

提出了适用于稠密预测（dense prediction）的空洞卷积（dilated convolution）模块。提出的这种模块系统性地整合了多尺度的上下文信息，同时又不损失分辨率。

## 思想

传统的图像分类网络通常通过连续的pooling或其他的下采样层来整合多尺度的上下文信息，这种方式会损失分辨率。而稠密预测不仅需要多尺度的上下文信息，同时还要求输出具有足够的分辨率。

为了解决这个矛盾问题，过去的论文的做法是：

1. 使用通过反卷积来恢复分辨率，同时通过下采样方法来扩大感受野（感受野越大，所具有的上下文信息越丰富）。对于这种方法，作者提出一个疑问：是否真的需要下采样层？
2. 提供多种重新放缩的图像（multiple rescaled verions of the image）作网络的输入。作者同样提出一个疑问：对重新放缩图像进行分别分析是否必要？

在这篇文章，作者提出了一种新的卷积网络模块，它能够整合多尺度的上下文信息，同时不丧失分辨率，也不需要分析重新放缩的图像。这种模块是为稠密预测专门设计的，它没有pooling或其他下采样。这个卷积网络模块是基于空洞卷积的，而空洞卷积是一种可以扩大感受野，同时不丧失分辨率。

## 基本概念

空洞卷积

本文中最核心的概念就是空洞卷积。不过空洞卷积的概念并不是由本篇文章提出的，它第一次出现在小波分解算法当中。并且，将其应用在卷积网络架构中这篇文章也不是第一个。空洞卷积的最大特点就是能够增大感受野，同时还不损失分辨率。下面看空洞卷积到底是什么。

首先看我们都很熟悉的卷积操作，如下动图。

![1](/home/stone/stone/notes/DeepLearning/segmentaion/notes/pics/1.gif)

这个卷积操作对原图做了填充（padding=1），卷积核大小为3x3，卷积的步长为2。

下面看空洞卷积：

![2](/home/stone/stone/notes/DeepLearning/segmentaion/notes/pics/2.gif)

这个空洞卷积操作的卷积核大小为3x3，空洞率（dilation rate）=2。卷积的步长为1。我们可以看到，标准的卷积操作中，卷积核的元素之间都是相邻的，但是在空洞卷积中，卷积核的元素是间隔的，间隔的大小取决于空洞率。所以，空洞卷积也没什么特别的。

那为什么空洞卷积能够扩大感受野并且保持分辨率呢？且看下面的这张图。