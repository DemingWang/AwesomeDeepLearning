# YOLO

[TOC]

整个系统如图所示：首先将输入图片resize到448x448，然后送入CNN网络，最后处理网络预测结果得到检测的目标。相比R-CNN算法，其是一个统一的框架，其速度更快，而且Yolo的训练过程也是end-to-end的。

![2](./pics/47.jpg)

具体来说，Yolo的CNN网络将输入的图片分割成 $S* S$ 网格，然后每个单元格负责去检测那些中心点落在该格子内的目标，如图所示，可以看到狗这个目标的中心落在左下角一个单元格内，那么该单元格负责预测这个狗。

预测分为两部分，bounding box的预测和分类预测。

## I.Bounding box预测

每个单元格会预测 $B$个**边界框**（bounding box）以及**边界框的置信度**（confidence score）。

所谓置信度其实包含两个方面，一是这个边界框含有目标的可能性大小，二是这个边界框的准确度。**前者记为 $Pr(object)$ ，当该边界框是背景时（即不包含目标），此时 $Pr(object)=0$。而当该边界框包含目标时， $Pr(object)=1$。**边界框的准确度可以用预测框与实际框（ground truth）的IOU（intersection over union，交并比）来表征，记为 ${IOU}^{truth}_{pred}$。因此置信度可以定义为 $Pr(object)*\text{IOU}^{truth}_{pred}$ 。很多人可能将Yolo的置信度看成边界框是否含有目标的概率，但是其实它是两个因子的乘积，预测框的准确度也反映在里面。

> **mensaochun注**：加粗部分表述不准确。要分训练和测试两个阶段来看待这个问题。当训练的时候，物体的中心落在该cell，那么此时$Pr(object)$就为1，对应的${IOU}^{truth}_{pred}=1$，此时置信度为1。否则$Pr(object)$就为0，这个时候置信度就为0。这也是训练的时候置信度的label。相反，如果是测试阶段，直接预测得到的实际上只有一个置信度，只是它隐式地包含了$Pr(object)*\text{IOU}^{truth}_{pred}$ 。

边界框的大小与位置可以用4个值来表征： $(x, y,w,h)$，其中 $(x,y)$ 是边界框的中心坐标，而 $w$和 $h $是边界框的宽与高。还有一点要注意，中心坐标的预测值 $(x,y)$ 是相对于每个单元格左上角坐标点的偏移值，并且单位是相对于单元格大小的，单元格的坐标定义如图6所示。而边界框的 $w$和 $h$ 预测值是相对于整个图片的宽与高的比例，这样理论上4个元素的大小应该在 $[0,1]$ 范围。这样，每个边界框的预测值实际上包含5个元素： $(x,y,w,h,c)$，其中前4个表征边界框的大小与位置，而最后一个值是置信度。

![2](./pics/48.jpg)

> mensaochun注：怎么计算偏移值？按照下图的方式计算。假设原图的大小为`w*h`, grid的size为S*S，某物体中心的坐标为（x,y），且其所在的cell左上角坐标为（a,b），如图箭头指向的cell的坐标为（5,0）。首先，x进行归一化，x/w，即将尺度缩小到[0,1]。然后扩大S倍，xS/w，将尺度扩大到[0,S]。最后，减去cell的横坐标，则得到最终的偏移值。总公式为：(xS/w-a,yS/h-b)。

![2](./pics/46.png)

## II.分类

还有分类问题，对于每一个单元格其还要给出预测出 $C$ 个类别概率值，其表征的是由该单元格负责预测的边界框其目标属于各个类别的概率。**但是这些概率值其实是在各个边界框置信度下的条件概率**（mensaochun注：这个部分表述是不准确的），即 $Pr(class_{i}|object)$ 。值得注意的是，不管一个单元格预测多少个边界框，其只预测一组类别概率值，这是Yolo算法的一个缺点，在后来的改进版本中，Yolo9000是把类别概率预测值与边界框是绑定在一起的。同时，我们可以计算出各个边界框类别置信度（class-specific confidence scores）:$Pr(class_{i}|object)*Pr(object)*\text{IOU}^{truth}_{pred}=Pr(class_{i})*\text{IOU}^{truth}_{pred}$。

> mensaochun注：这里也要分训练阶段和测试阶段来看，如果是训练阶段，首先看cell中是否有物体中心落入，如果有，再看落入的是什么物体，对应的$Pr(class_{i}|object)=1$，否则，相应的$Pr(class_{i}|object)=0$，这里也就体现了条件概率的意义。对于测试阶段， 我们还要将类别概率值乘以置信度。$Pr(class_{i}|object)*Pr(object)*\text{IOU}^{truth}_{pred}=Pr(class_{i})*\text{IOU}^{truth}_{pred}$。which gives us class-specific confidence scores for each box. These scores encode both the probability of that class appearing in the box and how well the predicted box fits the object。个人的理解，公式中应该是联合概率，如：$Pr(class_{i}|object)*Pr(object)*\text{IOU}^{truth}_{pred}=Pr(class_{i}，object)*\text{IOU}^{truth}_{pred}$，表示同时包含物体并且是某一类的可能性。

边界框类别置信度表征的是该边界框中目标属于各个类别的可能性大小以及边界框匹配目标的好坏。后面会说，一般会根据类别置信度来过滤网络的预测框。

总结一下，每个单元格需要预测 $(B*5+C)$个值。如果将输入图片划分为 $S\times S$网格，那么最终预测值为 $S\times S\times (B*5+C)$大小的张量。整个模型的预测值结构如下图所示。对于PASCAL VOC数据，其共有20个类别，如果使用 $S=7,B=2$ ，那么最终的预测结果就是 $7\times 7\times 30$ 大小的张量。在下面的网络结构中我们会详细讲述每个单元格的预测值的分布位置。

![2](./pics/49.jpg)

## III.网络设计

Yolo采用卷积网络来提取特征，然后使用全连接层来得到预测值。网络结构参考GooLeNet模型，包含24个卷积层和2个全连接层，如图8所示。对于卷积层，主要使用1x1卷积来做channle reduction，然后紧跟3x3卷积。对于卷积层和全连接层，采用Leaky ReLU激活函数： $max(x, 0.1x)$。但是最后一层却采用线性激活函数。

![2](./pics/50.jpg)

图8 网络结构

可以看到网络的最后输出为 $7\times 7\times 30$ 大小的张量。这和前面的讨论是一致的。这个张量所代表的具体含义如图9所示。对于每一个单元格，前20个元素是类别概率值，然后2个元素是边界框置信度，两者相乘可以得到类别置信度，最后8个元素是边界框的 $(x, y,w,h)$ 。大家可能会感到奇怪，对于边界框为什么把置信度 $c$和 $(x, y,w,h)$ 都分开排列，而不是按照 $(x, y,w,h,c)$这样排列，其实纯粹是为了计算方便，因为实际上这30个元素都是对应一个单元格，其排列是可以任意的。但是分离排布，可以方便地提取每一个部分。这里来解释一下，**首先网络的预测值是一个二维张量 $P$**，其shape为 $[batch, 7\times 7\times 30]$ 。采用切片，那么 $P_{[:,0:7*7*20]}$ 就是类别概率部分，而 $P_{[:,7*7*20:7*7*(20+2)]}$ 是置信度部分，最后剩余部分 $P_{[:,7*7*(20+2):]}$是边界框的预测结果。这样，提取每个部分是非常方便的，这会方面后面的训练及预测时的计算。

![2](./pics/53.jpg)

## VI.Loss设计

在训练之前，先在ImageNet上进行了预训练，其预训练的分类模型采用图8中前20个卷积层，然后添加一个average-pool层和全连接层。预训练之后，在预训练得到的20层卷积层之上加上随机初始化的4个卷积层和2个全连接层。由于检测任务一般需要更高清的图片，所以将网络的输入从224x224增加到了448x448。整个网络的流程如下图所示：

![2](./pics/51.jpg)

图10 Yolo网络流程

下面是训练损失函数的分析，Yolo算法将目标检测看成回归问题，所以采用的是均方差损失函数。但是对不同的部分采用了不同的权重值。首先区分定位误差和分类误差。对于定位误差，即边界框坐标预测误差，采用较大的权重 $\lambda _{coord}=5​$。然后其区分不包含目标的边界框与含有目标的边界框的置信度，对于前者，采用较小的权重值 $\lambda _{noobj}=0.5​$ 。其它权重值均设为1。然后采用均方误差，其同等对待大小不同的边界框，但是实际上较小的边界框的坐标误差应该要比较大的边界框要更敏感。为了保证这一点，将网络的边界框的宽与高预测改为对其平方根的预测，即预测值变为 $(x,y,\sqrt{w}, \sqrt{h})​$。

另外一点时，由于每个单元格预测多个边界框。但是其对应类别只有一个。那么在训练时，如果该单元格内确实存在目标，那么只选择与ground truth的IOU最大的那个边界框来负责预测该目标，而其它边界框认为不存在目标。这样设置的一个结果将会使一个单元格对应的边界框更加专业化，其可以分别适用不同大小，不同高宽比的目标，从而提升模型性能。大家可能会想如果一个单元格内存在多个目标怎么办，其实这时候Yolo算法就只能选择其中一个来训练，这也是Yolo算法的缺点之一。要注意的一点时，对于不存在对应目标的边界框，其误差项就是只有置信度，坐标项误差是没法计算的。而只有当一个单元格内确实存在目标时，才计算分类误差项，否则该项也是无法计算的。

综上讨论，最终的损失函数计算如下：

![2](./pics/52.jpg)

其中第一项是边界框中心坐标的误差项， $1^{obj}_{ij}$指的是第 $i$个单元格存在目标，且该单元格中的第 $j$个边界框负责预测该目标。第二项是边界框的高与宽的误差项。第三项是包含目标的边界框的置信度误差项。第四项是不包含目标的边界框的置信度误差项。而最后一项是包含目标的单元格的分类误差项， $1^{obj}_{i}$ 指的是第$i$个单元格存在目标。

> mensaochun注：
>
> 1.一定要注意一点，就是没有物体的时候，置信度的loss也要算进去。问题是，是否会因为置信度很多都很低，而影响网络呢？
>
> 2.还有另外一点，如果物体都集中在图像的某个部位，其他不出现物体的图像部位，其预测的bounding box的网络就得不到更新，这个会导致问题发生么？

## V.网络预测

下面就来分析Yolo的预测过程，这里我们不考虑batch，认为只是预测一张输入图片。根据前面的分析，最终的网络输出是 $7\times 7 \times 30$ ，但是我们可以将其分割成三个部分：类别概率部分为 $[7, 7, 20]$，置信度部分为 $[7,7,2]$，而边界框部分为 $[7,7,2,4]$（对于这部分不要忘记根据原始图片计算出其真实值）。然后将前两项相乘（矩阵 $[7, 7, 20]$乘以 $[7,7,2]$ 可以各补一个维度来完成 $[7,7,1,20]\times [7,7,2,1]$可以得到类别置信度值为 $[7, 7,2,20]$，这里总共预测了 $7*7*2=98个$边界框。

所有的准备数据已经得到了，那么我们先说第一种策略来得到检测框的结果，我认为这是最正常与自然的处理。首先，对于每个预测框根据类别置信度选取置信度最大的那个类别作为其预测标签，经过这层处理我们得到各个预测框的预测类别及对应的置信度值，其大小都是 ![[7,7,2]](https://www.zhihu.com/equation?tex=%5B7%2C7%2C2%5D) 。一般情况下，会设置置信度阈值，就是将置信度小于该阈值的box过滤掉，所以经过这层处理，剩余的是置信度比较高的预测框。最后再对这些预测框使用NMS算法，最后留下来的就是检测结果。一个值得注意的点是NMS是对所有预测框一视同仁，还是区分每个类别，分别使用NMS。Ng在deeplearning.ai中讲应该区分每个类别分别使用NMS，但是看了很多实现，其实还是同等对待所有的框，我觉得可能是不同类别的目标出现在相同位置这种概率很低吧。

上面的预测方法应该非常简单明了，但是对于Yolo算法，其却采用了另外一个不同的处理思路（至少从C源码看是这样的），其区别就是先使用NMS，然后再确定各个box的类别。其基本过程如图12所示。对于98个boxes，首先将小于置信度阈值的值归0，然后分类别地对置信度值采用NMS，这里NMS处理结果不是剔除，而是将其置信度值归为0。最后才是确定各个box的类别，当其置信度值不为0时才做出检测结果输出。这个策略不是很直接，但是貌似Yolo源码就是这样做的。Yolo论文里面说NMS算法对Yolo的性能是影响很大的，所以可能这种策略对Yolo更好。但是我测试了普通的图片检测，两种策略结果是一样的。

![2](./pics/54.jpg)

图12 Yolo的预测处理流程

 